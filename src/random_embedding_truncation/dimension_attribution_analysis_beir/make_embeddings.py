import torch
import json, pathlib, sys, os
import faiss
from tqdm import tqdm
import numpy as np
from sentence_transformers import SentenceTransformer
import zlib
import gc

from random_embedding_truncation.truncator import Truncator

# Load embedding model (You can change to another model if needed)
# ì„ë² ë”© ìƒì„±
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Which Device : {device}")
model_name = 'all-MiniLM-L6-v2'
st_model = SentenceTransformer(model_name, device=device)
is_e5 = "e5" in model_name.lower()

# Truncator ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (resize_scale=1.0ìœ¼ë¡œ ì„¤ì •í•˜ë©´ truncation ì—†ìŒ)
truncator_model = Truncator(st_model, resize_scale=1.0, is_e5=is_e5)

def batch_encoding(emb_mem, off, nxt_off, corpus_batch):
    # Truncatorì˜ encode_corpus ë©”ì„œë“œ ì‚¬ìš©
    embeddings = truncator_model.encode_corpus(
        corpus_batch, 
        batch_size=128, 
        convert_to_numpy=True, 
        show_progress_bar=True, 
        num_workers=3
    )
    print(f"text len : {embeddings.shape[0]}, offset_size : {nxt_off - off}")

    if embeddings.shape[0] == (nxt_off - off):
        print("offset match")
        emb_mem[off:nxt_off] = embeddings
    else:
        print("offset mismatch")
        del emb_mem
        sys.exit(1)
    
def read_corpus(corpus_path, dataset_name):
    # Read JSON lines file and process
    # Truncator.encode_corpusê°€ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ list[dict] í˜•íƒœë¡œ ë°˜í™˜
    corpus = []

    print("File Open Start")
    with open(corpus_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line.strip())
            corpus.append(data)

    print("File Read Complete")
    return corpus

def make_embeddings(dataset_name):
    # corpus file path
    corpus_filename = "corpus.jsonl"

    # ëª¨ë¸ ì´ë¦„ì„ íŒŒì¼ ì‹œìŠ¤í…œì— ì•ˆì „í•œ í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: "sentence-transformers/all-MiniLM-L6-v2" -> "sentence-transformers-all-MiniLM-L6-v2")
    model_name_safe = model_name.replace("/", "-")

    # dataset and embedding directory
    # cache_dir êµ¬ì¡°ì— ë§ì¶°ì„œ: {cache_dir}/{dataset_name}/ í˜•íƒœë¡œ ì €ì¥
    dataset_dir = os.path.join("/media/dcceris/muvera_optimized/datasets", dataset_name)
    # cache_dirì€ model_nameì„ í¬í•¨í•œ ê²½ë¡œ (ì˜ˆ: /media/dcceris/muvera_optimized/embeddings/all-MiniLM-L6-v2)
    cache_dir = os.path.join("/media/dcceris/muvera_optimized/embeddings", model_name_safe)
    embedding_dir = os.path.join(cache_dir, dataset_name)
    os.makedirs(embedding_dir, exist_ok=True)
    os.makedirs(dataset_dir, exist_ok=True)

    # corpus file path and embedding file path
    file_path = os.path.join(dataset_dir, corpus_filename)
    embedding_file = os.path.join(embedding_dir, f"{dataset_name}_{model_name_safe}_embeddings.dat")

    # Convert texts to embeddings
    print("start encode")
    if pathlib.Path(embedding_file).exists():
        print(f"âœ… [INFO] ì„ë² ë”© íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤. íŒŒì¼ì„ ì½ìŠµë‹ˆë‹¤: {embedding_file}")
        return embedding_file
    else:
        print(f"âœ… [INFO] ì„ë² ë”© íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ batch ìƒì„±í•©ë‹ˆë‹¤: {embedding_file}")

        # Read JSON lines file and process
        corpus = read_corpus(file_path, dataset_name)

        tot_doc = len(corpus)
        d = truncator_model.st.get_sentence_embedding_dimension()
        print(f"total doc: {tot_doc}, dimension: {d}")

        emb_mem = np.memmap(embedding_file, dtype="float32", mode="w+", shape=(tot_doc, d))
        batch_size = 20000
        offset = 0

        print(f"âœ… [INFO] ì„ë² ë”© íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ batch ìƒì„±í•©ë‹ˆë‹¤: {embedding_file}")
        while (offset < tot_doc):
            next_offset = min(offset + batch_size, tot_doc)
            print(f" offset : {offset}, offset + B : {next_offset - 1}, next_offset  : {next_offset}") 
            corpus_batch = corpus[offset:next_offset]
            batch_encoding(emb_mem, offset, next_offset, corpus_batch)
            offset = next_offset
        
        del emb_mem
        del corpus
        gc.collect()
        print("encode complete")
        print(f"âœ… [INFO] ì„ë² ë”© íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {embedding_file}")

        return embedding_file

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("âŒ ì‚¬ìš©ë²•: [arg0]python3 [arg1]dataset name [arg2] cluster size [arg3] make_index.py")
        sys.exit(1)

    dataset_name = sys.argv[1]



    try:
        # Make IVF index
        make_embeddings(dataset_name)

        gc.collect()

    except Exception as e:
        print(f"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)